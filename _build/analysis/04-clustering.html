---
interact_link: content/analysis/04-clustering.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Clustering
prev_page:
  url: /analysis/03-dimensionality-reduction.html
  title: |-
    Dimensionality reduction
next_page:
  url: /analysis/05-diffexp.html
  title: |-
    Differential expression
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<main class="jupyter-page">

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Clustering">Clustering<a class="anchor-link" href="#Clustering"> </a></h1><h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2><p>Grouping cells based on the similarity of their expression profiles allows us to identify cell types and states, as well as infer differences between groups. This is done either via <strong>clustering</strong> or <strong>community detection</strong>.</p>
<p><strong>Unsupervised clustering</strong> is useful in many different applications and has been widely studied in machine learning. Some of the most popular approaches are <strong>hierarchical clustering</strong> and <strong>k-means clustering</strong>. These methods compute a <strong>distance metric</strong> between cells (often based on a low-dimensional representation, such as PCA, tSNE or UMAP), and then iteratively group cells together based on these distances.</p>
<p><strong>Community detection</strong> methods (also referred to as 'graph-based clustering') partition the <strong>neighbor graph</strong>. The neighbor graph treats each cell as a node, with edges connecting each node to its <em>k</em> nearest neighbors (based on similar distance metrics described above). The graph is then partitioned into modules based on these connectivities. These methods are typically faster than other clustering methods with similar effectiveness.</p>
<p>One thing virtually all clustering or community detection methods have in common is some flavor of a <strong>resolution parameter</strong>. This parameter controls how fine- or coarse-grained the inferred clusters are. This parameter can have major effects on your results! We'll explore this in more detail below.</p>
<p>Here, we'll explore <strong>k-means clustering</strong> and the graph-based <strong>louvain clustering</strong> method.</p>
<h2 id="Load-data">Load data<a class="anchor-link" href="#Load-data"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">scanpy</span> <span class="k">as</span> <span class="nn">sc</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">adjusted_rand_score</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">adata</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s1">&#39;../data/brain_embeddings.h5ad&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="k-means">k-means<a class="anchor-link" href="#k-means"> </a></h2><p>In <a href="https://en.wikipedia.org/wiki/K-means_clustering"><em>k</em>-means clustering</a>, the goal is to partition <em>N</em> cells into <em>k</em>
different clusters. This is done in an iterative manner, cluster centers are
assigned and each cell is assigned to its nearest cluster:</p>
<p><img src="../figures/k-means.png" alt=""></p>
<p>Let's try this out on the umap representation of our dataset. Scanpy doesn't include a method for k-means clustering, so we'll extract the umap coordinates that we calculated earlier and use <code>scikit-learn</code> for this task instead.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">umap_coordinates</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obsm</span><span class="p">[</span><span class="s1">&#39;X_umap&#39;</span><span class="p">]</span> <span class="c1"># extract the UMAP coordinates for each cell</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">umap_coordinates</span><span class="p">)</span> <span class="c1"># fix the random state for reproducibility</span>

<span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;kmeans&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span> <span class="c1"># retrieve the labels and add them as a metadata column in our AnnData object</span>
<span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;kmeans&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;kmeans&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

<span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">)</span> <span class="c1"># plot the results</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>... storing &#39;kmeans&#39; as categorical
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/analysis/04-clustering_3_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluating-clustering">Evaluating clustering<a class="anchor-link" href="#Evaluating-clustering"> </a></h2><p>Intuitively, we can see from the plot that our value of <em>k</em> (the number of clusters) is probably too low.</p>
<p>This dataset has "ground truth" cell type labels available. We can use these to assess our cluster labels a bit more rigorously using the <strong>adjusted Rand index</strong>. This index is a measure between (0, 1) which indicates the similarity between two sets of categorical labels (e.g., our cell type labels and cluster labels). A value of 1 means the two clusterings are identical, and 0 means the level of similarity expected by random chance.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rand_index</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;cell_ontology_class&#39;</span><span class="p">],</span> <span class="n">labels_pred</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;kmeans&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The Rand index is&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">rand_index</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The Rand index is 0.85
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exercise">Exercise<a class="anchor-link" href="#Exercise"> </a></h3><p>Try rerunning k-means clustering with several different values of <em>k</em>. Does this improve the Rand index?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Graph-based-methods">Graph-based methods<a class="anchor-link" href="#Graph-based-methods"> </a></h2><p>Graph-based methods attempt to partition a pre-computed <strong>neighhbor graph</strong> into modules (i.e., groups / clusters of cells) based on their connectivity. Currently, the most widely used graph-based methods for single cell data are variants of the <strong>louvain</strong> algorithm. The intuition behind the louvain algorithm is that it looks for areas of the neighbor graph that are more densely connected than expected (based on the overall connectivity in the graph).</p>
<p><img width=500 src='../figures/graph_network.jpg'></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exercise">Exercise<a class="anchor-link" href="#Exercise"> </a></h2><ol>
<li><p>Use the scanpy function <code>sc.tl.louvain</code> to compute the graph-based cluster labels for our dataset. Visualize your results on the UMAP plot. What is the Rand index compared to the ground-truth cell types?</p>
</li>
<li><p>Repeat this with several different values for the <code>resoltion</code> parameter. How does this effect your results?</p>
</li>
</ol>
<p><p></p>
<details>
<summary><h3>Solution</h3></summary>
<code>
sc.tl.louvain(adata)
sc.pl.umap(adata, color='louvain')
rand_index = adjusted_rand_score(adata.obs['cell_ontology_class'], adata.obs['louvain'])
print('The rand index is ', round(rand_index, 2))</code>
<p>    
The low rand index with the default resolution parameter is quite low (0.34), but this increases to 0.9 when we set <code>resolution=0.1</code>.  
</details>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exercise">Exercise<a class="anchor-link" href="#Exercise"> </a></h2><p>In this dataset, we're lucky enough to have carefully curated cell type labels to help guide our choice of clustering method and parameters. However, we may not always have this ground truth available; deciding what constitutes a reasonable clustering result is a judgement call. In some cases, this is largely dependent upon how choose to define "cell type."</p>
<ol>
<li>What do you think is a reasonable definition of "cell type?" Why? How could this guide your choice of cluster parameters? Brainstorm independently for 2 minutes, then share with your neighbor. Do you agree? </li>
</ol>
<ol>
<li>Use the adjusted Rand index to compare the labels from k-means clustering to the labels from louvain clustering. How similar are they?</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Advanced-exercise">Advanced exercise<a class="anchor-link" href="#Advanced-exercise"> </a></h2><p>So far, we've explored how the choice of resolution parameter influences the results we get from clustering. However, these clustering algorithms are also downstream dependents on the results of umap (k-means and louvain) and the neighbor graph (louvain). These methods also have parameter choices that can influence our results.</p>
<p>Try recalculating <code>sc.tl.umap</code> and <code>sc.pp.neighbors</code> under different parameter values (remember that you can use <code>help(function)</code> to see the documentation of each argument the function accepts). Then recalculate clusters with your favorite method. How much did this change the results?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Advanced-exercise">Advanced exercise<a class="anchor-link" href="#Advanced-exercise"> </a></h2><p>Sometimes, we may want to look at clusters within a given tissue or cell type designation. This can surface interesting heterogeneity between subpopulations, although it can also make our results more noisy.</p>
<p>Use the snippet below to subset the data to cells from the cerebellum and recalculate the neighbor graph and umap embedding for this subset. Then rerun your favorite clustering method on these cells. What do you observe? Do you think these subpopulations are based on real biological variation or primarily due to noise? How can you tell?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cerebellum</span> <span class="o">=</span> <span class="n">adata</span><span class="p">[</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;subtissue&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Cerebellum&#39;</span><span class="p">]</span>
<span class="n">sc</span><span class="o">.</span><span class="n">pp</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">cerebellum</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">cerebellum</span><span class="p">)</span>

<span class="c1"># your favorite clustering method here</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's finalize our clusters and write to file</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">louvain</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">adata</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;../data/brain_clusters.h5ad&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

 


</main>
